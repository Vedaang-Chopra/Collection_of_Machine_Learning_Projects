{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# This file creates the feature set for the documents given.\n",
    "def creating_dataset(vocab,output_labels,word_array,freq_array):\n",
    "    # main_content=[]\n",
    "    content=[[0 for j in range(len(vocab))] for i in range(len(output_labels))]\n",
    "    for i in range(len(word_array)):\n",
    "        for j in range(len((word_array[i]))):\n",
    "            for k in range(len(vocab)):\n",
    "                if word_array[i][j]==vocab[k]:\n",
    "                    content[i][k]=freq_array[i][j]\n",
    "                else:\n",
    "                    continue\n",
    "    return content\n",
    "\n",
    "# The following creates the vocabulary for all the words.\n",
    "def forming_vocab(word_array,freq_array):\n",
    "    val=4\n",
    "    freq_array=np.array(freq_array)\n",
    "    word_array = np.array(word_array)\n",
    "    # freq_array_copy=freq_array.copy()\n",
    "    # for i in range(len(freq_array)):\n",
    "    #     freq_array_copy[i].sort()\n",
    "    # print(freq_array_copy)\n",
    "    temp_holder=[]\n",
    "    # print(freq_array.shape[0])\n",
    "    for i in range(len(freq_array)):\n",
    "        for j in range(len(freq_array[i])):\n",
    "            # print(freq_array[i][j])\n",
    "            if freq_array[i][j]<=val:\n",
    "                temp_holder.append((i,j))\n",
    "            else:\n",
    "                continue\n",
    "    vocab=[]\n",
    "    for i in range((word_array).shape[0]):\n",
    "        for j in range(len(word_array[i])):\n",
    "            temp=(i,j)\n",
    "            if temp in temp_holder:\n",
    "                continue\n",
    "            else:\n",
    "                vocab.append(word_array[i][j])\n",
    "    return vocab\n",
    "\n",
    "# The function reads all the files word by word, finds the frequency of each word, and appends the word into an array\n",
    "# so that the top k words can be found for creating vocabulary.\n",
    "def reading_files(class_label):\n",
    "    pathname = os.path.dirname(sys.argv[0])\n",
    "    path_temp=os.path.abspath(pathname)\n",
    "    path_main = path_temp+'\\\\Train_Data\\\\'\n",
    "    path_main = path_main + class_label\n",
    "    os.chdir(path_main)\n",
    "    stop_words_list = stopwords.words(\"english\")\n",
    "    path_main = path_main + '\\\\'\n",
    "    files = os.listdir()\n",
    "    word_array = []\n",
    "    freq_array = []\n",
    "    for k in range(4):\n",
    "        fr = open(path_main + str(files[k]), 'r')\n",
    "        string = fr.read()\n",
    "        string.strip()\n",
    "        word = \"\"\n",
    "        for i in range(len(string)):\n",
    "            if string[i] == \" \" or string[i] == \",\" or string[i] == \":\" or string[i] == \"\\\"\" or string[i] == \"(\" or string[i] == \")\" or string[i] == \"|\" or string[i] == \"-\" or string[i] == \".\" or string[i] == \"{\" or string[i] == \"}\" or string[i] == \"[\" or string[i] == \"]\" or string[i] == \"@\" or string[i] == \"\\n\" or string[i] == \">\" or string[i] == \"<\" or string[i] == \"!\":\n",
    "                # print(word)\n",
    "                word=(((word).strip()).lower())\n",
    "                if word in stop_words_list:\n",
    "                    word=\"\"\n",
    "                elif word==\" \" or word==\"\" or word==\",\" or word==\":\" or word==\"\\\"\" or word==\"(\" or word==\")\" or word==\"|\" or word==\".\" or word==\";\" or word==\"[\" or word==\"]\" or word==\">\" or word==\"<\" or word==\"\\n\" or word==\"!\" :\n",
    "                    word=\"\"\n",
    "                elif word == \"{\" or word == \"}\" or word == \">=\" or word == \"=>\" or word == \"!=\" or word == \"<=\" or word == \"=<\" or word == \"*\" or word == \"+\" or word == \"/\" or word == \"-\" or word == \"?\" or word == \"/\" or word == \"\\\\\" or word == \"@\" or word==\"=\" or word==\">\" or word==\"<\" or word==\"\\'\" :\n",
    "                    word = \"\"\n",
    "                elif len(word)==1:\n",
    "                    if ord(word)>=65 and ord(word)<=90:\n",
    "                        word=\"\"\n",
    "                    elif ord(word)>=97 and ord(word)<=122:\n",
    "                        word=\"\"\n",
    "                    elif ord(word)>=48 and ord(word)<=57:\n",
    "                        word=\"\"\n",
    "                    elif word== \"\\n\":\n",
    "                        word=\"\"\n",
    "                    else:\n",
    "                        word=\"\"\n",
    "                elif word in word_array:\n",
    "                    for j in range(len(word_array)):\n",
    "                        if word_array[j] == word:\n",
    "                            freq_array[j] = freq_array[j] + 1\n",
    "                            word = \"\"\n",
    "                            break\n",
    "                else:\n",
    "                    word_array.append(word)\n",
    "                    freq_array.append(1)\n",
    "                    word = \"\"\n",
    "            else:\n",
    "                word = word + string[i]\n",
    "\n",
    "        # print(len(word_array))\n",
    "        # print(freq_array)\n",
    "        fr.close()\n",
    "    return word_array, freq_array\n",
    "\n",
    "\n",
    "def fetching_data(output_labels):\n",
    "    word_array=[[] for i in range(len(output_labels))]\n",
    "    freq_array = [[] for i in range(len(output_labels))]\n",
    "    for i in range(len(output_labels)):\n",
    "        word_array[i],freq_array[i]=(reading_files(output_labels[i]))\n",
    "    # check_vocab(word_array,freq_array)\n",
    "    return word_array,freq_array\n",
    "\n",
    "# Splitting the training and testing Data..............\n",
    "def train_test_split(y_train):\n",
    "    # Creates 500 test files ..........................\n",
    "    for i in range(500):\n",
    "        random_no = random.randint(0, 19)\n",
    "        pathname = os.path.dirname(sys.argv[0])\n",
    "        path_temp = os.path.abspath(pathname)\n",
    "        path_main = path_temp + '\\\\Train_data'\n",
    "        moving = path_temp+'\\\\test_data'\n",
    "        path_main = path_main + '\\\\'\n",
    "        moving=moving+'\\\\'\n",
    "        path_main = path_main + y_train[random_no]\n",
    "        moving = moving + y_train[random_no]\n",
    "        os.chdir(path_main)\n",
    "        path_main = path_main + '\\\\'\n",
    "        moving = moving + '\\\\'\n",
    "        files = os.listdir()\n",
    "        no = random.randint(0, len(files))  # Randomizing file selection\n",
    "        x_test_file_path = path_main + str(files[no])\n",
    "        moving_path=moving+str(files[no])\n",
    "        shutil.move(x_test_file_path, moving_path)\n",
    "\n",
    "def load():\n",
    "    # print(\"First File working\")\n",
    "    output_labels = ['alt.atheism',\n",
    "                     'comp.graphics',\n",
    "                     'comp.os.ms-windows.misc',\n",
    "                     'comp.sys.ibm.pc.hardware',\n",
    "                     'comp.sys.mac.hardware',\n",
    "                     'comp.windows.x',\n",
    "                     'misc.forsale',\n",
    "                     'rec.autos',\n",
    "                     'rec.motorcycles',\n",
    "                     'rec.sport.baseball',\n",
    "                     'rec.sport.hockey',\n",
    "                     'sci.crypt',\n",
    "                     'sci.electronics',\n",
    "                     'sci.med',\n",
    "                     'sci.space',\n",
    "                     'soc.religion.christian',\n",
    "                     'talk.politics.guns',\n",
    "                     'talk.politics.mideast',\n",
    "                     'talk.politics.misc',\n",
    "                     'talk.religion.misc']\n",
    "\n",
    "    # print(ord(\"x\"))\n",
    "    train_test_split(output_labels)\n",
    "    word_array,freq_array=fetching_data(output_labels)\n",
    "    # stop_words_list = stopwords.words(\"english\")\n",
    "    vocab=forming_vocab(word_array,freq_array)\n",
    "    # print(len(vocab))\n",
    "    content=creating_dataset(vocab,output_labels,word_array,freq_array)\n",
    "    x=[]\n",
    "    x.append(vocab)\n",
    "    for i in range(len(content)):\n",
    "        x.append(content[i])\n",
    "    # x=pd.DataFrame\n",
    "    # x.columns=[vocab[i] for i in range(len(vocab))]\n",
    "    x=np.array(x)\n",
    "    # print(x.shape)\n",
    "    return x,output_labels\n",
    "\n",
    "\n",
    "\n",
    "# The following function finds the p(i)/probability of each group/class.\n",
    "def basic_probability_calc(class_label):\n",
    "    basic_prob=np.zeros(len(class_label))\n",
    "    for i in range(len(class_label)):\n",
    "        pathname = os.path.dirname(sys.argv[0])\n",
    "        path_temp = os.path.abspath(pathname)\n",
    "        path_main = path_temp+'\\\\Train_Data\\\\'\n",
    "        path_main = path_main + class_label[i]\n",
    "        os.chdir(path_main)\n",
    "        path_main = path_main + '\\\\'\n",
    "        files = os.listdir()\n",
    "        # print(len(files))\n",
    "        basic_prob[i]=len(files)\n",
    "    basic_prob=basic_prob/basic_prob.sum()\n",
    "    return basic_prob\n",
    "\n",
    "# The following function implements the multinomial bayes theorem for any file.\n",
    "# It divides the count of that word(found in x_test file) by the corresponding sum of all the words for that group/class.\n",
    "# It then adds the log of each division and gives the result by adding with the log of the probabiltiy of that class.\n",
    "# It then returns the max probabilty class.\n",
    "def naive_bayes(x_train,y_train,x_test):\n",
    "    # print(len(x_train))\n",
    "    prob = basic_probability_calc(y_train)\n",
    "    prob=np.array(prob)\n",
    "    for i in prob:\n",
    "        i=math.log(i,2)\n",
    "    c=np.zeros(len(x_train)-1)\n",
    "    sum=np.zeros(len(x_train)-1)\n",
    "    temp=0\n",
    "    # The corresponding loop finds the sum of all words corresponding to that label.\n",
    "    for i in range(1,len(x_train)):\n",
    "        for j in range(len(x_train[i])):\n",
    "            sum[i-1]=sum[i-1]+int(x_train[i][j])\n",
    "    # print(sum)\n",
    "    # print(x_test)\n",
    "    # print(x_train)\n",
    "    for i in range(1,len(x_train)):\n",
    "        # print(\"Next Output Label\")\n",
    "        for j in range(len(x_test)):\n",
    "            if x_test[j] > 0:\n",
    "                # print(temp,x_test[j],x_train[i][j],sum[i-1])\n",
    "                temp_value=((int(x_train[i][j])+1)/(sum[i-1]+(x_train.shape[1])))\n",
    "                temp=temp+math.log(temp_value,2)\n",
    "        c[i-1]=temp\n",
    "        temp=0\n",
    "    c=prob+c\n",
    "    max_value=max(c)\n",
    "    # print(c)\n",
    "    for i in range(len(c)):\n",
    "        if c[i]==max_value:\n",
    "            return y_train[i]\n",
    "\n",
    "# The following Function is used to clean the x_test file by finding the words and their frequency from the files\n",
    "# group by group(Group here represents a single type of output).\n",
    "def creating_words(file_path):\n",
    "    stop_words_list = stopwords.words(\"english\")\n",
    "    word_array = []\n",
    "    freq_array = []\n",
    "    fr = open(file_path, 'r')   # Opening x_test file\n",
    "    string = fr.read()\n",
    "    string.strip()\n",
    "    word = \"\"\n",
    "    # The following for loop reads file word by word, removing the stop words and punctuations.\n",
    "    for i in range(len(string)):\n",
    "        if string[i] == \" \" or string[i] == \",\" or string[i] == \":\" or string[i] == \"\\\"\" or string[i] == \"(\" or string[\n",
    "            i] == \")\" or string[i] == \"|\" or string[i] == \"-\" or string[i] == \".\" or string[i] == \"{\" or string[\n",
    "            i] == \"}\" or string[i] == \"[\" or string[i] == \"]\" or string[i] == \"@\" or string[i] == \"\\n\" or string[\n",
    "            i] == \">\" or string[i] == \"<\" or string[i] == \"!\":\n",
    "            # print(word)\n",
    "            word = (((word).strip()).lower())\n",
    "            if word in stop_words_list:\n",
    "                word = \"\"\n",
    "            elif word == \" \" or word == \"\" or word == \",\" or word == \":\" or word == \"\\\"\" or word == \"(\" or word == \")\" or word == \"|\" or word == \".\" or word == \";\" or word == \"[\" or word == \"]\" or word == \">\" or word == \"<\" or word == \"\\n\" or word == \"!\":\n",
    "                word = \"\"\n",
    "            elif word == \"{\" or word == \"}\" or word == \">=\" or word == \"=>\" or word == \"!=\" or word == \"<=\" or word == \"=<\" or word == \"*\" or word == \"+\" or word == \"/\" or word == \"-\" or word == \"?\" or word == \"/\" or word == \"\\\\\" or word == \"@\" or word == \"=\" or word == \">\" or word == \"<\" or word == \"\\'\":\n",
    "                word = \"\"\n",
    "            elif len(word) == 1:\n",
    "                if ord(word) >= 65 and ord(word) <= 90:\n",
    "                    word = \"\"\n",
    "                elif ord(word) >= 97 and ord(word) <= 122:\n",
    "                    word = \"\"\n",
    "                elif ord(word) >= 48 and ord(word) <= 57:\n",
    "                    word = \"\"\n",
    "                elif word == \"\\n\":\n",
    "                    word = \"\"\n",
    "                else:\n",
    "                    word = \"\"\n",
    "            elif word in word_array:\n",
    "                for j in range(len(word_array)):\n",
    "                    if word_array[j] == word:\n",
    "                        freq_array[j] = freq_array[j] + 1\n",
    "                        word = \"\"\n",
    "                        break\n",
    "            else:\n",
    "                word_array.append(word)\n",
    "                freq_array.append(1)\n",
    "                word = \"\"\n",
    "        else:\n",
    "            word = word + string[i]\n",
    "\n",
    "    # print(len(word_array))\n",
    "    # print(freq_array)\n",
    "    fr.close()\n",
    "    return word_array,freq_array\n",
    "\n",
    "# The following functions checks for the words belonging to the x_test file and compares with them to the words with\n",
    "# the vocabulary of the x_train. It then takes the count of the word common to the file and x_train vocabulary.\n",
    "def finding_count(vocab,word_array,freq_array):\n",
    "    content = [0 for j in range(len(vocab))]\n",
    "    for i in range(len(word_array)):\n",
    "        for k in range(len(vocab)):\n",
    "            if word_array[i]==vocab[k]:\n",
    "                content[k]=freq_array[i]\n",
    "            else:\n",
    "                continue\n",
    "    return content\n",
    "\n",
    "\n",
    "def cleaning_file(file_path,x):\n",
    "    word_array,freq_array=creating_words(file_path)\n",
    "    # print(x[0])\n",
    "    count=finding_count(x[0],word_array,freq_array)\n",
    "    return count\n",
    "\n",
    "# print(\"Working Main function\")\n",
    "# Code Starts Here.........................\n",
    "x_train,y_train=load()                # Working on the x_train/Getting  documents into a 2-D array form\n",
    "print(x_train.shape)\n",
    "right,wrong=0,0\n",
    "# The following Code tests on 40 Random files from each group\n",
    "for i in range(len(y_train)):\n",
    "    pathname = os.path.dirname(sys.argv[0])\n",
    "    path_temp = os.path.abspath(pathname)\n",
    "    path_main = path_temp+'\\\\test_data'\n",
    "    path_main = path_main + '\\\\'\n",
    "    path_main= path_main + y_train[i]\n",
    "    os.chdir(path_main)\n",
    "    path_main = path_main + '\\\\'\n",
    "    files = os.listdir()\n",
    "    for j in range(len(files)):\n",
    "        x_test_file_path=path_main+str(files[j])\n",
    "        count=cleaning_file(x_test_file_path,x_train)\n",
    "        y_pred=naive_bayes(x_train,y_train,count)   # Calling the multinomial naive bayes\n",
    "        print(y_pred,'Predicted Y for the File Path using own code......................')\n",
    "        print(y_train[i], 'Actual Y for the File Path......................')\n",
    "        if (y_pred==y_train[i]):\n",
    "            right=right+1\n",
    "        else:\n",
    "            wrong=wrong+1\n",
    "print(\"No. of right and wrong Predictions:\",right,\"     \",wrong)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
